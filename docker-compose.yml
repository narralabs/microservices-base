services:
  frontend:
    build:
      context: ./src/frontend
    ports:
      - "3000:3000"
    environment:
      - USER_SERVICE_URL=user-service:7000
      - CHAT_SERVICE_URL=chat-service:50051
      - CART_SERVICE_URL=cart-service:7002
      - SESSION_SECRET=your-super-secret-key-change-in-production
    command: nodemon ./bin/www
    volumes:
      - ./src/frontend:/app
      - /app/node_modules
    depends_on:
      - user-service
      - chat-service
      - cart-service

  user-service:
    build:
      context: ./src/user-service
    ports:
      - "7000"
    environment:
      - MONGODB_URL=mongodb://mongo-service:27017/microservices_development
    command: nodemon server.js
    volumes:
      - ./src/user-service:/app
      - /app/node_modules
    depends_on:
      - mongo-service

  cart-service:
    build:
      context: ./src/cart-service
    ports:
      - "7002"
    environment:
      - MONGODB_URL=mongodb://mongo-service:27017/microservices_development
    command: nodemon server.js
    volumes:
      - ./src/cart-service:/app
      - /app/node_modules
    depends_on:
      - mongo-service

  mongo-service:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db

  llama-service:
    build:
      context: ./src/llama-service
    ports:
      - "8080:8080"
    volumes:
      - ./src/llama-service/models:/models
    environment:
      - N_GPU_LAYERS=0
      - N_CTX=2048
      - N_THREADS=4
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          memory: 4G
          cpus: '2'
    command: [
      "--model", "/models/model.gguf",
      "--host", "0.0.0.0", "--port", "8080",
      "--n-gpu-layers", "0",
      "--ctx-size", "2048",
      "--threads", "4"]

  chat-service:
    build:
      context: ./src/chat-service
    ports:
      - "50051"
    volumes:
      - ./src/chat-service:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - LLAMA_SERVICE_URL=http://llama-service:8080
    depends_on:
      - llama-service
    command: ["nodemon", "server.js"]

volumes:
  mongodb_data: